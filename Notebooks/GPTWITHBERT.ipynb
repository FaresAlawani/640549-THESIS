{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem.snowball import DutchStemmer\n",
    "import matplotlib.pyplot as plt\n",
    "from wordcloud import WordCloud\n",
    "import tiktoken\n",
    "import json\n",
    "from openai import OpenAI\n",
    "import time\n",
    "import psutil\n",
    "import tracemalloc\n",
    "from bertopic import BERTopic\n",
    "import nbformat\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "import umap as UMAP\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###Topic_info is extracted output of BERTopic model runs\n",
    "r1 = pd.read_csv('topic_infoR13.csv')\n",
    "r2 = pd.read_csv('topic_infoR2.csv')\n",
    "r3 = pd.read_csv('topic_infoR3.csv')\n",
    "r4 = pd.read_csv('topic_infoR4.csv')\n",
    "r5 = pd.read_csv('topic_infoR5.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r1 = r1[['Representative_Docs']]\n",
    "r2 = r2[['Representative_Docs']]\n",
    "r3 = r3[['Representative_Docs']]\n",
    "r4 = r4[['Representative_Docs']]\n",
    "r5 = r5[['Representative_Docs']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = OpenAI(api_key='xxxxxxxxxxxxx')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Topic modeling function using GPT-3.5-turbo\n",
    "def topic_modeling(example):\n",
    "  response = client.chat.completions.create(\n",
    "    model=\"gpt-3.5-turbo-0125\",\n",
    "    messages=[\n",
    "      {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": \"You are an expert in topic modeling customer review data. I am going to provide you with a dataset of customer reviews. Please analyze this data and return a detailed mapping of the most recurring topics. For each topic, list the most associated keywords that represent it. Ensure that the topics and keywords are clear, distinct, and capture the essence of the reviews.\" \n",
    "      },\n",
    "      {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": f\"{example}\"\n",
    "      }\n",
    "    ],\n",
    "    temperature=1,\n",
    "    top_p=1,\n",
    "    frequency_penalty=0,\n",
    "    presence_penalty=0\n",
    "  )\n",
    "  \n",
    "  try:\n",
    "    return response.choices[0].message.content\n",
    "  except (KeyError, IndexError) as e:\n",
    "    return \"Error: Unable to process the response.\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#####This process was run iteritavely for each retailer\n",
    "# Get the current process\n",
    "process = psutil.Process()\n",
    "\n",
    "    # Measure initial CPU times and memory usage\n",
    "cpu_times_before = process.cpu_times()\n",
    "memory_usage_before = process.memory_info().rss\n",
    "\n",
    "    # Start measuring peak memory usage\n",
    "tracemalloc.start()\n",
    "\n",
    "    # Measure start time\n",
    "start_time = time.perf_counter()\n",
    "\n",
    "    # Apply topic modeling to each chunk\n",
    "r5['topics'] = r5['Representative_Docs'].apply(topic_modeling)\n",
    "\n",
    "    # Measure end time\n",
    "end_time = time.perf_counter()\n",
    "\n",
    "    # Measure final memory usage\n",
    "memory_usage_after = process.memory_info().rss\n",
    "\n",
    "    # Stop measuring peak memory usage and get the result\n",
    "current, peak = tracemalloc.get_traced_memory()\n",
    "tracemalloc.stop()\n",
    "\n",
    "    # Measure final CPU times\n",
    "cpu_times_after = process.cpu_times()\n",
    "\n",
    "    # Calculate execution time\n",
    "execution_time = end_time - start_time\n",
    "\n",
    "    # Calculate CPU usage\n",
    "user_time = cpu_times_after.user - cpu_times_before.user\n",
    "system_time = cpu_times_after.system - cpu_times_before.system\n",
    "cpu_usage = ((user_time + system_time) / execution_time) * 100 / psutil.cpu_count()\n",
    "\n",
    "    # Calculate memory usage difference\n",
    "memory_usage_difference = (memory_usage_after - memory_usage_before) / (1024 ** 2)  # Convert bytes to MB\n",
    "\n",
    "    # Calculate peak memory usage\n",
    "memory_usage_peak = peak / (1024 ** 2)  # Convert bytes to MB\n",
    "\n",
    "    # Print the results\n",
    "print(f\"Execution time for r: {execution_time} seconds\")\n",
    "print(f\"CPU usage during execution for r: {cpu_usage}%\")\n",
    "print(f\"Peak memory usage during execution for r: {memory_usage_peak} MB\")\n",
    "print(f\"Memory usage difference during execution for r: {memory_usage_difference} MB\")\n",
    "\n",
    "#########################\n",
    "r5[['topics']].to_html('R5gptbert.html')\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
