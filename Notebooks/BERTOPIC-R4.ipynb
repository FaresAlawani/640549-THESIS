{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####pacakges\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from umap import UMAP\n",
    "from hdbscan import HDBSCAN\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from bertopic.vectorizers import ClassTfidfTransformer\n",
    "from bertopic.representation import KeyBERTInspired\n",
    "from bertopic import BERTopic\n",
    "from transformers import pipeline\n",
    "from sklearn.decomposition import PCA\n",
    "import openai\n",
    "import time\n",
    "import psutil\n",
    "import tracemalloc\n",
    "from bertopic.representation import OpenAI\n",
    "import tiktoken\n",
    "from bertopic.representation import MaximalMarginalRelevance\n",
    "\n",
    "nltk.download('stopwords')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dutch_stopwords = stopwords.words('dutch')\n",
    "\n",
    "###\n",
    "client = openai.OpenAI(api_key=\"xxxxxxxxxxxxxx\")\n",
    "\n",
    "# Tokenizer\n",
    "tokenizer= tiktoken.encoding_for_model(\"gpt-3.5-turbo\")\n",
    "\n",
    "# Create your representation model\n",
    "\n",
    "GPT = OpenAI(\n",
    "    client,\n",
    "    model=\"gpt-3.5-turbo\", \n",
    "    delay_in_seconds=2, \n",
    "    chat=True,\n",
    "    nr_docs=4,\n",
    "    doc_length=100,\n",
    "    tokenizer=tokenizer\n",
    ")\n",
    "######\n",
    "aspect_model2 = [KeyBERTInspired(top_n_words=20), MaximalMarginalRelevance(diversity=.5)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set dispalay \n",
    "pd.set_option('display.max_rows', 100)        \n",
    "pd.set_option('display.max_columns', 20)       \n",
    "pd.set_option('display.max_colwidth', 100)     \n",
    "pd.set_option('display.width', 1000)          \n",
    "pd.set_option('display.precision', 3)          \n",
    "pd.set_option('display.expand_frame_repr', False)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##########################\n",
    "r4 = pd.read_csv('R4FINALPROCESS.csv')\n",
    "##################\n",
    "r4 = r4['Review'].tolist() \n",
    "##################\n",
    "r4 = [str(doc) for doc in r4]  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "process = psutil.Process()\n",
    "cpu_times_before = process.cpu_times()\n",
    "memory_usage_before = process.memory_info().rss\n",
    "\n",
    "tracemalloc.start()\n",
    "start_time = time.perf_counter()\n",
    "\n",
    "#############################\n",
    "embedding_model = SentenceTransformer(\"paraphrase-multilingual-MiniLM-L12-v2\")\n",
    "embeddings = embedding_model.encode(r4, show_progress_bar=True)\n",
    "\n",
    "###########################\n",
    "end_time = time.perf_counter()\n",
    "memory_usage_after = process.memory_info().rss\n",
    "current, peak = tracemalloc.get_traced_memory()\n",
    "tracemalloc.stop()\n",
    "\n",
    "cpu_times_after = process.cpu_times()\n",
    "execution_time = end_time - start_time\n",
    "user_time = cpu_times_after.user - cpu_times_before.user\n",
    "system_time = cpu_times_after.system - cpu_times_before.system\n",
    "cpu_usage = ((user_time + system_time) / execution_time) * 100 / psutil.cpu_count()\n",
    "memory_usage_difference = (memory_usage_after - memory_usage_before) / (1024 ** 2)\n",
    "memory_usage_peak = peak / (1024 ** 2)\n",
    "\n",
    "print(f\"Execution time: {execution_time} seconds\")\n",
    "print(f\"CPU usage during execution: {cpu_usage}%\")\n",
    "print(f\"Peak memory usage during execution: {memory_usage_peak} MB\")\n",
    "print(f\"Memory usage difference during execution: {memory_usage_difference} MB\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rescale(x, inplace=False):\n",
    "    \"\"\" Rescale an embedding so optimization will not have convergence issues.\n",
    "    \"\"\"\n",
    "    if not inplace:\n",
    "        x = np.array(x, copy=True)\n",
    "\n",
    "    x /= np.std(x[:, 0]) * 10000\n",
    "\n",
    "    return x\n",
    "\n",
    "# Step 2: Prepare for Resource Monitoring\n",
    "process = psutil.Process()\n",
    "cpu_times_before = process.cpu_times()\n",
    "memory_usage_before = process.memory_info().rss\n",
    "\n",
    "tracemalloc.start()\n",
    "start_time = time.perf_counter()\n",
    "##############################\n",
    "np.random.seed(28)\n",
    "\n",
    "\n",
    "# Initialize and rescale PCA embeddings\n",
    "pca_embeddings = rescale(PCA(n_components=5).fit_transform(embeddings))\n",
    "\n",
    "#######################\n",
    "end_time = time.perf_counter()\n",
    "memory_usage_after = process.memory_info().rss\n",
    "current, peak = tracemalloc.get_traced_memory()\n",
    "tracemalloc.stop()\n",
    "\n",
    "cpu_times_after = process.cpu_times()\n",
    "execution_time = end_time - start_time\n",
    "user_time = cpu_times_after.user - cpu_times_before.user\n",
    "system_time = cpu_times_after.system - cpu_times_before.system\n",
    "cpu_usage = ((user_time + system_time) / execution_time) * 100 / psutil.cpu_count()\n",
    "memory_usage_difference = (memory_usage_after - memory_usage_before) / (1024 ** 2)\n",
    "memory_usage_peak = peak / (1024 ** 2)\n",
    "\n",
    "print(f\"Execution time: {execution_time} seconds\")\n",
    "print(f\"CPU usage during execution: {cpu_usage}%\")\n",
    "print(f\"Peak memory usage during execution: {memory_usage_peak} MB\")\n",
    "print(f\"Memory usage difference during execution: {memory_usage_difference} MB\")\n",
    "###############"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "process = psutil.Process()\n",
    "cpu_times_before = process.cpu_times()\n",
    "memory_usage_before = process.memory_info().rss\n",
    "\n",
    "tracemalloc.start()\n",
    "start_time = time.perf_counter()\n",
    "##############################\n",
    "\n",
    "umap_model = UMAP(n_neighbors=10, n_components=5, min_dist=0.0, metric='cosine', init=pca_embeddings, random_state=42)\n",
    "\n",
    "# Vectorization\n",
    "vectorizer_model = CountVectorizer(ngram_range=(1, 3), stop_words=dutch_stopwords)\n",
    "\n",
    "# Topic Representation\n",
    "ctfidf_model = ClassTfidfTransformer(bm25_weighting=True, reduce_frequent_words=True)\n",
    "\n",
    "\n",
    "#######################\n",
    "end_time = time.perf_counter()\n",
    "memory_usage_after = process.memory_info().rss\n",
    "current, peak = tracemalloc.get_traced_memory()\n",
    "tracemalloc.stop()\n",
    "\n",
    "cpu_times_after = process.cpu_times()\n",
    "execution_time = end_time - start_time\n",
    "user_time = cpu_times_after.user - cpu_times_before.user\n",
    "system_time = cpu_times_after.system - cpu_times_before.system\n",
    "cpu_usage = ((user_time + system_time) / execution_time) * 100 / psutil.cpu_count()\n",
    "memory_usage_difference = (memory_usage_after - memory_usage_before) / (1024 ** 2)\n",
    "memory_usage_peak = peak / (1024 ** 2)\n",
    "\n",
    "print(f\"Execution time: {execution_time} seconds\")\n",
    "print(f\"CPU usage during execution: {cpu_usage}%\")\n",
    "print(f\"Peak memory usage during execution: {memory_usage_peak} MB\")\n",
    "print(f\"Memory usage difference during execution: {memory_usage_difference} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \n",
    "process = psutil.Process()\n",
    "cpu_times_before = process.cpu_times()\n",
    "memory_usage_before = process.memory_info().rss\n",
    "\n",
    "tracemalloc.start()\n",
    "start_time = time.perf_counter()\n",
    "##############################\n",
    "\n",
    "# Fine-tune parameters for clustering\n",
    "\n",
    "# Define the parameter ranges\n",
    "min_cluster_sizes = np.arange(165, 326, 20)\n",
    "min_samples_values = [3, 4, 5, 15, 30, 50, 100, 150]\n",
    "\n",
    "for min_cluster_size in min_cluster_sizes:\n",
    "    for min_samples in min_samples_values:\n",
    "        # Update HDBSCAN model\n",
    "        hdbscan_model = HDBSCAN(min_cluster_size=min_cluster_size, min_samples=min_samples, metric='euclidean', prediction_data=True)\n",
    "\n",
    "        # Create a BERTopic instance with the updated HDBSCAN model\n",
    "        topic_model = BERTopic(embedding_model=embedding_model,\n",
    "                               umap_model=umap_model,\n",
    "                               hdbscan_model=hdbscan_model,\n",
    "                               vectorizer_model=vectorizer_model,\n",
    "                               ctfidf_model=ctfidf_model,\n",
    "                               language=\"multilingual\",\n",
    "                               nr_topics=\"auto\")\n",
    "\n",
    "        # Train the model\n",
    "        topics, probs = topic_model.fit_transform(r4, embeddings)\n",
    "\n",
    "        # Evaluate the model\n",
    "        num_outliers = topics.count(-1)\n",
    "        num_topics = len(set(topics)) - (1 if -1 in topics else 0)\n",
    "\n",
    "        # Print the results for each iteration\n",
    "        print(f\"Iteration with min_cluster_size={min_cluster_size}, min_samples={min_samples}\")\n",
    "        print(f\"Number of outliers: {num_outliers}\")\n",
    "        print(f\"Number of topics: {num_topics}\")\n",
    "        print(\"-------------------------------------------------\")\n",
    "\n",
    "#######################\n",
    "end_time = time.perf_counter()\n",
    "memory_usage_after = process.memory_info().rss\n",
    "current, peak = tracemalloc.get_traced_memory()\n",
    "tracemalloc.stop()\n",
    "\n",
    "cpu_times_after = process.cpu_times()\n",
    "execution_time = end_time - start_time\n",
    "user_time = cpu_times_after.user - cpu_times_before.user\n",
    "system_time = cpu_times_after.system - cpu_times_before.system\n",
    "cpu_usage = ((user_time + system_time) / execution_time) * 100 / psutil.cpu_count()\n",
    "memory_usage_difference = (memory_usage_after - memory_usage_before) / (1024 ** 2)\n",
    "memory_usage_peak = peak / (1024 ** 2)\n",
    "\n",
    "print(f\"Execution time: {execution_time} seconds\")\n",
    "print(f\"CPU usage during execution: {cpu_usage}%\")\n",
    "print(f\"Peak memory usage during execution: {memory_usage_peak} MB\")\n",
    "print(f\"Memory usage difference during execution: {memory_usage_difference} MB\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MIN_CLUSTER_SIZE = 205\n",
    "MIN_SAMPLES= 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keybert_model = KeyBERTInspired()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reducing dimensional space\n",
    "umap_model = UMAP(n_neighbors=10, n_components=5, min_dist=0.0, metric='cosine', init=pca_embeddings, random_state=42)\n",
    "\n",
    "# Clustering\n",
    "hdbscan_model = HDBSCAN(metric='euclidean', prediction_data=True)\n",
    "\n",
    "# Vectorization\n",
    "vectorizer_model = CountVectorizer(ngram_range=(1, 3), stop_words=dutch_stopwords)\n",
    "\n",
    "# Topic Representation\n",
    "ctfidf_model = ClassTfidfTransformer(bm25_weighting=True, reduce_frequent_words=True)\n",
    "\n",
    "# Representation Model\n",
    "representation_model = {\n",
    "    \"main\": keybert_model,\n",
    "    \"aspect1\": GPT,\n",
    "   \"aspect2\": aspect_model2 }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MODEL FITTING\n",
    "process = psutil.Process()\n",
    "cpu_times_before = process.cpu_times()\n",
    "memory_usage_before = process.memory_info().rss\n",
    "\n",
    "tracemalloc.start()\n",
    "start_time = time.perf_counter()\n",
    "##############################\n",
    "\n",
    "topic_model = BERTopic(\n",
    "\n",
    "  # Pipeline models\n",
    "  embedding_model=embedding_model,\n",
    "  umap_model=umap_model,\n",
    "  hdbscan_model=hdbscan_model,\n",
    "  vectorizer_model=vectorizer_model,\n",
    "  ctfidf_model=ctfidf_model,\n",
    "  representation_model=representation_model,\n",
    "\n",
    "  # Hyperparameters\n",
    "  language = \"multilingual\",\n",
    "  top_n_words=10,\n",
    "  calculate_probabilities=True,\n",
    "  nr_topics=\"auto\",\n",
    "  verbose=True\n",
    ")\n",
    "\n",
    "np.random.seed(42)\n",
    "# Train model\n",
    "topics, probs = topic_model.fit_transform(r4, embeddings)\n",
    "\n",
    "#######################\n",
    "end_time = time.perf_counter()\n",
    "memory_usage_after = process.memory_info().rss\n",
    "current, peak = tracemalloc.get_traced_memory()\n",
    "tracemalloc.stop()\n",
    "\n",
    "cpu_times_after = process.cpu_times()\n",
    "execution_time = end_time - start_time\n",
    "user_time = cpu_times_after.user - cpu_times_before.user\n",
    "system_time = cpu_times_after.system - cpu_times_before.system\n",
    "cpu_usage = ((user_time + system_time) / execution_time) * 100 / psutil.cpu_count()\n",
    "memory_usage_difference = (memory_usage_after - memory_usage_before) / (1024 ** 2)\n",
    "memory_usage_peak = peak / (1024 ** 2)\n",
    "\n",
    "print(f\"Execution time: {execution_time} seconds\")\n",
    "print(f\"CPU usage during execution: {cpu_usage}%\")\n",
    "print(f\"Peak memory usage during execution: {memory_usage_peak} MB\")\n",
    "print(f\"Memory usage difference during execution: {memory_usage_difference} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get topic info\n",
    "topic_info = topic_model.get_topic_info()\n",
    "#\n",
    "topic_info = pd.DataFrame(topic_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_info.head(17)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#####Extracting output for further analysis\n",
    "topic_info[['Representation']].to_html('bert4.html')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###Hierarchical visualization for merging topics \n",
    "hierarchical_topics = topic_model.hierarchical_topics(r4)\n",
    "fig = visualize_hierarchy(topic_model, hierarchical_topics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define topics to merge as a list of lists\n",
    "topics_to_merge = [[2, 5], [6, 7], [4, 9]] \n",
    "\n",
    "# Function to merge topics in BERTopic\n",
    "def merge_topics_bertopic(topic_model, documents, topics_to_merge):\n",
    "    for topic_group in topics_to_merge:\n",
    "        if len(topic_group) > 1:\n",
    "            # Merge topics in the group\n",
    "            new_topic = topic_model.merge_topics(documents, topic_group)\n",
    "    return topic_model\n",
    "\n",
    "# Merge the topics\n",
    "topic_model = merge_topics_bertopic(topic_model, r4, topics_to_merge)\n",
    "\n",
    "# Get the updated topic information\n",
    "topic_info = topic_model.get_topic_info()\n",
    "\n",
    "# Convert the topic information to a DataFrame\n",
    "df_topic_info = pd.DataFrame(topic_info)\n",
    "\n",
    "# Export the DataFrame to a CSV file\n",
    "csv_file_path = 'topic_infoR4.csv'\n",
    "df_topic_info.to_csv(csv_file_path, index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
